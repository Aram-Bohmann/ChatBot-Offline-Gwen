# ğŸ¤– ChatBot Offline com Qwen2

[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)
[![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama&logoColor=white)](https://ollama.ai/)
[![Qwen2](https://img.shields.io/badge/Qwen2-5C2D91?style=for-the-badge&logo=ai&logoColor=white)](https://qwen.readthedocs.io/)

> **Assistente de IA Conversacional 100% Local com Privacidade Total**  
> Processamento offline, sem dependÃªncia de APIs externas ou conexÃ£o Ã  internet

ChatBot inteligente desenvolvido em Python com Streamlit e Ollama, utilizando o modelo Qwen2 para conversas naturais completamente offline, garantindo mÃ¡xima privacidade e autonomia.

---

## ğŸ“– Sobre o Projeto

**ChatBot Pessoal Offline** Ã© uma soluÃ§Ã£o de IA conversacional que roda **100% localmente** em sua mÃ¡quina, sem enviar dados para servidores externos. Ideal para quem valoriza privacidade, trabalha com informaÃ§Ãµes sensÃ­veis ou precisa de IA em ambientes sem internet.

### ğŸ¯ PropÃ³sito

Mais do que um chatbot comum, o sistema oferece:
- ğŸ”’ **Privacidade Total** - Nenhum dado sai da sua mÃ¡quina
- âš¡ **Sem Custos de API** - Zero dependÃªncia de serviÃ§os pagos
- ğŸŒ **Funciona Offline** - Internet nÃ£o Ã© necessÃ¡ria apÃ³s instalaÃ§Ã£o
- ğŸ’¬ **Conversas Naturais** - Modelo Qwen2 com alta qualidade de resposta
- ğŸ“ **HistÃ³rico Persistente** - MantÃ©m contexto durante toda a sessÃ£o
- ğŸ¨ **Interface Intuitiva** - Design clean e responsivo com Streamlit

---

## âœ¨ Funcionalidades Principais

### ğŸ’¬ **Chat Conversacional**

#### Interface Interativa
- Input de texto estilo chat moderno
- HistÃ³rico de mensagens com identificaÃ§Ã£o visual (usuÃ¡rio/assistente)
- RenderizaÃ§Ã£o Markdown para respostas formatadas
- Container estilizado com bordas para melhor organizaÃ§Ã£o

#### Gerenciamento de SessÃ£o
- **Session State** do Streamlit para persistÃªncia de contexto
- HistÃ³rico completo de conversas mantido durante a sessÃ£o
- Mensagens anteriores visÃ­veis para referÃªncia

#### Processamento de IA
- IntegraÃ§Ã£o direta com Ollama via Python Client
- Modelo Qwen2 (latest) para geraÃ§Ã£o de respostas
- Tratamento de erros com mensagens amigÃ¡veis
- Respostas completas geradas de uma vez

---

## ğŸ—ï¸ Arquitetura do Sistema

### Fluxo de Funcionamento
```mermaid
graph TD
    A[UsuÃ¡rio digita mensagem] --> B[Streamlit captura input]
    B --> C[Adiciona ao histÃ³rico]
    C --> D[Exibe mensagem do usuÃ¡rio]
    D --> E[Envia para Ollama]
    E --> F[Modelo Qwen2 processa]
    F --> G[Gera resposta]
    G --> H[Exibe resposta do assistente]
    H --> I[Adiciona resposta ao histÃ³rico]
    I --> J[Aguarda nova mensagem]
```

### Componentes TÃ©cnicos

#### 1. **Frontend (Streamlit)**
- `st.set_page_config()` - ConfiguraÃ§Ã£o da pÃ¡gina (tÃ­tulo, layout)
- `st.container(border=True)` - Container estilizado para chat
- `st.chat_message()` - Componente nativo de mensagens de chat
- `st.chat_input()` - Input otimizado para conversaÃ§Ã£o
- `st.session_state` - Gerenciamento de estado da sessÃ£o

#### 2. **Backend (Ollama Client)**
- `ollama.Client()` - Cliente Python para Ollama
- `client.generate()` - MÃ©todo de geraÃ§Ã£o de texto
- Modelo: `qwen2:latest` - LLM de cÃ³digo aberto

#### 3. **Estrutura de Dados**

**Session State - HistÃ³rico de Mensagens:**
```python
st.session_state.messages = [
    {"role": "user", "content": "OlÃ¡, tudo bem?"},
    {"role": "assistant", "content": "OlÃ¡! Estou bem, obrigado..."}
]
```

**Resposta do Ollama:**
```python
response_data = {
    'response': 'Texto gerado pelo modelo...',
    'model': 'qwen2:latest',
    'created_at': '2025-02-10T...',
    # ... outros metadados
}
```

---

## ğŸš€ Como Executar

### PrÃ©-requisitos

#### 1. **Python 3.11 ou Superior**
```bash
python --version
# SaÃ­da esperada: Python 3.11.x ou superior
```

#### 2. **Ollama Instalado**

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**macOS:**
```bash
brew install ollama
```

**Windows:**
- Baixe o instalador em [ollama.ai](https://ollama.ai/download)

**Verificar instalaÃ§Ã£o:**
```bash
ollama --version
```

#### 3. **Modelo Qwen2**
```bash
# Baixar o modelo (~ 4.7GB)
ollama pull qwen2:latest

# Verificar modelos instalados
ollama list
```

---

### InstalaÃ§Ã£o do Projeto

#### OpÃ§Ã£o 1: InstalaÃ§Ã£o Direta
```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/chatbot-qwen2-offline.git

# Entre no diretÃ³rio
cd chatbot-qwen2-offline

# Instale as dependÃªncias
pip install streamlit ollama
```

#### OpÃ§Ã£o 2: Com Ambiente Virtual (Recomendado)
```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/chatbot-qwen2-offline.git
cd chatbot-qwen2-offline

# Crie o ambiente virtual
python -m venv venv

# Ative o ambiente
# Linux/macOS:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# Instale as dependÃªncias
pip install streamlit ollama
```

#### OpÃ§Ã£o 3: Com Poetry
```bash
# Instale Poetry (se nÃ£o tiver)
pip install poetry

# Clone e entre no diretÃ³rio
git clone https://github.com/seu-usuario/chatbot-qwen2-offline.git
cd chatbot-qwen2-offline

# Instale dependÃªncias
poetry install

# Ative o ambiente Poetry
poetry shell
```

---

### Executar o ChatBot

#### 1. **Inicie o ServiÃ§o Ollama**
```bash
# Em um terminal separado
ollama serve
```

#### 2. **Execute o Streamlit**
```bash
# Terminal principal
streamlit run app.py

# O navegador abrirÃ¡ automaticamente em:
# http://localhost:8501
```

#### 3. **Teste a ConexÃ£o**
Digite qualquer mensagem no chat e aguarde a resposta!

---

## ğŸ› ï¸ Stack TecnolÃ³gica

### Core
![Python](https://img.shields.io/badge/Python_3.11-3776AB?style=flat-square&logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit_1.31+-FF4B4B?style=flat-square&logo=streamlit&logoColor=white)

### IA & Machine Learning
![Ollama](https://img.shields.io/badge/Ollama-000000?style=flat-square&logo=ollama&logoColor=white)
![Qwen2](https://img.shields.io/badge/Qwen2_Latest-5C2D91?style=flat-square&logo=ai&logoColor=white)

### DevOps
![Poetry](https://img.shields.io/badge/Poetry-60A5FA?style=flat-square&logo=poetry&logoColor=white)
![Git](https://img.shields.io/badge/Git-F05032?style=flat-square&logo=git&logoColor=white)
![VS Code](https://img.shields.io/badge/VS_Code-007ACC?style=flat-square&logo=visual-studio-code&logoColor=white)

---

## ğŸ’¡ Casos de Uso

### ğŸ”’ **Empresa com Dados SensÃ­veis**

**CenÃ¡rio:** EscritÃ³rio de advocacia precisa de assistente IA para rascunhar documentos sem expor informaÃ§Ãµes confidenciais

**SoluÃ§Ã£o:**
1. Deploy do ChatBot em servidor interno
2. Processamento 100% local sem conexÃ£o externa
3. HistÃ³rico de conversas mantido apenas na sessÃ£o

**Resultado:**
- âœ… Conformidade total com LGPD/GDPR
- âœ… Zero risco de vazamento de dados
- âœ… Custo zero com APIs de IA

---

### ğŸŒ **Trabalho em Ambientes Offline**

**CenÃ¡rio:** Pesquisador de campo sem acesso Ã  internet precisa de assistente IA

**SoluÃ§Ã£o:**
1. InstalaÃ§Ã£o prÃ©via do Ollama e modelo Qwen2
2. ChatBot funcionando offline
3. Respostas rÃ¡pidas sem latÃªncia de rede

**Resultado:**
- âš¡ Produtividade mantida sem internet
- ğŸ’° Economia com planos de dados mÃ³veis
- ğŸ”‹ Menor consumo de bateria (sem requisiÃ§Ãµes HTTP)

---

### ğŸ“ **EducaÃ§Ã£o e Aprendizado**

**CenÃ¡rio:** Estudante quer praticar programaÃ§Ã£o com IA sem custos

**SoluÃ§Ã£o:**
1. InstalaÃ§Ã£o gratuita do ChatBot
2. Perguntas ilimitadas sem cobranÃ§a por token
3. ExperimentaÃ§Ã£o sem medo de custos

**Resultado:**
- ğŸ“š Aprendizado contÃ­nuo sem limites
- ğŸ’µ Zero gastos com GPT-4/Claude API
- ğŸ§ª Liberdade para testar e errar

---

### ğŸ¨ **CustomizaÃ§Ãµes PossÃ­veis**

#### 1. **Alterar o Modelo**
```python
# Trocar para outro modelo Ollama
response_data = client.generate(
    model="llama2:latest",  # ou "mistral", "codellama", etc.
    prompt=prompt
)
```

#### 2. **Adicionar Temperatura**
```python
response_data = client.generate(
    model="qwen2:latest",
    prompt=prompt,
    options={
        "temperature": 0.7,  # 0.0 = determinÃ­stico, 1.0 = criativo
        "top_p": 0.9,
        "top_k": 40
    }
)
```

#### 3. **Streaming de Respostas**
```python
# Exibir texto enquanto Ã© gerado
response_placeholder = st.empty()
full_response = ""

for chunk in client.generate(model="qwen2:latest", prompt=prompt, stream=True):
    full_response += chunk['response']
    response_placeholder.markdown(full_response)
```

#### 4. **Sistema de Prompt**
```python
system_message = "VocÃª Ã© um assistente Ãºtil e educado."

response_data = client.generate(
    model="qwen2:latest",
    prompt=prompt,
    system=system_message
)
```

---

## ğŸš€ Melhorias Futuras

### Roadmap

#### Curto Prazo
- [ ] **BotÃ£o de limpar histÃ³rico** de conversas
- [ ] **Salvar conversas** em arquivo JSON/TXT
- [ ] **Seletor de modelos** no sidebar (Qwen2, Llama2, Mistral)
- [ ] **ConfiguraÃ§Ã£o de temperatura** via slider

#### MÃ©dio Prazo
- [ ] **Streaming de respostas** palavra por palavra
- [ ] **Upload de documentos** para contexto
- [ ] **Busca no histÃ³rico** de conversas
- [ ] **Temas customizÃ¡veis** (claro/escuro)

#### Longo Prazo
- [ ] **RAG (Retrieval-Augmented Generation)** com ChromaDB
- [ ] **Multi-sessÃµes** com banco de dados SQLite
- [ ] **AnÃ¡lise de sentimento** das conversas
- [ ] **Export para PDF** com formataÃ§Ã£o

---


## ğŸ“ Contexto AcadÃªmico

### InformaÃ§Ãµes do Projeto

| Item | Detalhe |
|------|---------|
| **Tipo** | Projeto de PortfÃ³lio - IA Conversacional |
| **Tecnologias** | Python, Streamlit, Ollama, Qwen2 |
| **Categoria** | Natural Language Processing (NLP) |
| **Complexidade** | IntermediÃ¡ria |

### CompetÃªncias Demonstradas

1. **ğŸ¤– IA Generativa** - IntegraÃ§Ã£o com LLMs locais
2. **ğŸ Python AvanÃ§ado** - ManipulaÃ§Ã£o de APIs, gerenciamento de estado
3. **ğŸ¨ UI/UX** - Interface conversacional intuitiva
4. **ğŸ”’ SeguranÃ§a** - Processamento local de dados sensÃ­veis
5. **ğŸ“¦ Deployment** - ConfiguraÃ§Ã£o de ambiente, dependÃªncias
6. **ğŸ“ DocumentaÃ§Ã£o** - README tÃ©cnico completo
7. **ğŸ› ï¸ DevOps** - Troubleshooting, otimizaÃ§Ã£o de performance

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas!

### Como Contribuir

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/melhoria`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona streaming de respostas'`)
4. Push para a branch (`git push origin feature/melhoria`)
5. Abra um Pull Request

### Ãreas de ContribuiÃ§Ã£o

- ğŸ¤– **Modelos** - IntegraÃ§Ã£o com novos LLMs
- ğŸ¨ **UI/UX** - Melhorias visuais e de usabilidade
- ğŸ“Š **Features** - Novas funcionalidades (RAG, multi-sessÃ£o)
- ğŸ› **Bugs** - CorreÃ§Ãµes e otimizaÃ§Ãµes
- ğŸ“ **Docs** - Melhorias na documentaÃ§Ã£o

---

## ğŸ“ LicenÃ§a

Este projeto foi desenvolvido para fins **educacionais e de portfÃ³lio** e estÃ¡ disponÃ­vel para:

âœ… Uso educacional e estudo  
âœ… ModificaÃ§Ã£o e adaptaÃ§Ã£o  
âœ… Uso em projetos pessoais e comerciais  
âœ… DistribuiÃ§Ã£o com crÃ©ditos  

âš ï¸ **AtenÃ§Ã£o:** O modelo Qwen2 possui licenÃ§a prÃ³pria da Alibaba Cloud. Consulte [qwen.readthedocs.io](https://qwen.readthedocs.io/) para detalhes.

---

## ğŸ“ Contato

**Desenvolvedor:** [Aram Bohmann Leite Da Luz]  

[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:arambohmannleitedaluz@gmail.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/aram-luz-1b0ab1321/)
[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Aram-Bohmann)

---

## ğŸ™ Agradecimentos

- **Streamlit** - Framework incrÃ­vel para apps de ML/IA
- **Ollama** - Tornando LLMs locais acessÃ­veis
- **Alibaba Cloud** - Modelo Qwen2 de cÃ³digo aberto
- **Comunidade Python** - Ferramentas e suporte
- **Open Source Community** - Por democratizar IA

---

## ğŸ“š ReferÃªncias TÃ©cnicas

- [Streamlit Documentation](https://docs.streamlit.io/)
- [Ollama Documentation](https://ollama.ai/docs)
- [Qwen2 Model Card](https://qwen.readthedocs.io/)
- [Python Ollama Library](https://github.com/ollama/ollama-python)
- [LLM Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)

---

## ğŸ”— Links Ãšteis

- [Ollama Models Library](https://ollama.ai/library) - CatÃ¡logo de modelos disponÃ­veis
- [Streamlit Gallery](https://streamlit.io/gallery) - InspiraÃ§Ã£o para apps
- [Awesome Ollama](https://github.com/jmorganca/awesome-ollama) - Recursos da comunidade
- [Qwen2 Paper](https://arxiv.org/abs/2407.10671) - Artigo cientÃ­fico do modelo

---

<div align="center">

### â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!

**Desenvolvido com ğŸ’™ e ğŸ¤– para IA acessÃ­vel e privada**

*"IA poderosa nÃ£o precisa de internet"*

</div>
